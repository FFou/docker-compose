# 从Kafka读取日志消息
input {
    kafka {
        zk_connect => "localhost:2181"
        group_id => "logstash"
        topic_id => "test"
        reset_beginning => false
        consumer_threads => 1
        queue_size => 20
        rebalance_max_retries => 4
        rebalance_backoff_ms => 2000
        consumer_timeout_ms => -1
        consumer_restart_on_error => true
        consumer_restart_sleep_ms => 0
        decorate_events => false
        consumer_id => nil
        fetch_message_max_bytes => 1048576
    }
}

# 将消息发送到Kafka
output {
    kafka {
        broker_list => "localhost:9092"
        topic_id => "test"
        compression_codec => "none"
        compressed_topics => ""
        request_required_acks => 0 # [-1, 0, 1]
        serializer_class => "kafka.serializer.StringEncoder"
        partitioner_class => "kafka.producer.DefaultPartitioner"
        request_timeout_ms => 10000
        producer_type => 'sync'
        key_serializer_class => "kafka.serializer.StringEncoder"
        message_send_max_retries => 3
        retry_backoff_ms => 100
        topic_metadata_refresh_interval_ms => 600 * 1000
        queue_buffering_max_ms => 5000
        queue_buffering_max_messages => 10000
        queue_enqueue_timeout_ms => -1
        batch_num_messages => 200
        send_buffer_bytes => 100 * 1024
        client_id => ""
    }
}